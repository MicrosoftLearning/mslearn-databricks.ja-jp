---
lab:
  title: Azure Databricks を探索する
---

# Azure Databricks を探索する

Azure Databricks は、一般的なオープンソース Databricks プラットフォームの Microsoft Azure ベースのバージョンです。

Azure Synapse Analytics と同様に、Azure Databricks の "ワークスペース" は、Azure 上の Databricks クラスター、データ、およびリソースを管理するための中心点を提供します。**

この演習の所要時間は約 **30** 分です。

## Azure Databricks ワークスペースをプロビジョニングする

> **ヒント**: 既に Azure Databricks ワークスペースがある場合は、この手順をスキップして、既存のワークスペースを使用できます。

この演習には、新しい Azure Databricks ワークスペースをプロビジョニングするスクリプトが含まれています。 このスクリプトは、この演習で必要なコンピューティング コアに対する十分なクォータが Azure サブスクリプションにあるリージョンに、*Premium* レベルの Azure Databricks ワークスペース リソースを作成しようとします。また、使用するユーザー アカウントのサブスクリプションに、Azure Databricks ワークスペース リソースを作成するための十分なアクセス許可があることを前提としています。 十分なクォータやアクセス許可がないためにスクリプトが失敗した場合は、Azure portal で、Azure Databricks ワークスペースを対話形式で作成してみてください。

1. Web ブラウザーで、`https://portal.azure.com` の [Azure portal](https://portal.azure.com) にサインインします。
2. ページ上部の検索バーの右側にある **[\>_]** ボタンを使用して、Azure portal に新しい Cloud Shell を作成します。メッセージが表示されたら、***PowerShell*** 環境を選んで、ストレージを作成します。 次に示すように、Azure portal の下部にあるペインに、Cloud Shell のコマンド ライン インターフェイスが表示されます。

    ![Azure portal と Cloud Shell のペイン](./images/cloud-shell.png)

    > **注**: 前に *Bash* 環境を使ってクラウド シェルを作成している場合は、そのクラウド シェル ペインの左上にあるドロップダウン メニューを使って、***PowerShell*** に変更します。

3. ペインの上部にある区分線をドラッグして Cloud Shell のサイズを変更したり、ペインの右上にある **&#8212;** 、 **&#9723;** 、**X** アイコンを使用して、ペインを最小化または最大化したり、閉じたりすることができます。 Azure Cloud Shell の使い方について詳しくは、[Azure Cloud Shell のドキュメント](https://docs.microsoft.com/azure/cloud-shell/overview)をご覧ください。

4. PowerShell のペインで、次のコマンドを入力して、リポジトリを複製します。

    ```
    rm -r mslearn-databricks -f
    git clone https://github.com/MicrosoftLearning/mslearn-databricks
    ```

5. リポジトリをクローンした後、次のコマンドを入力して **setup.ps1** スクリプトを実行します。これにより、使用可能なリージョンに Azure Databricks ワークスペースがプロビジョニングされます。

    ```
    ./mslearn-databricks/setup.ps1
    ```

6. メッセージが表示された場合は、使用するサブスクリプションを選択します (これは、複数の Azure サブスクリプションへのアクセス権を持っている場合にのみ行います)。
7. スクリプトの完了まで待ちます。通常、約 5 分かかりますが、さらに時間がかかる場合もあります。 待っている間に、Azure Databricks ドキュメントの記事「[Azure Databricks とは](https://learn.microsoft.com/azure/databricks/introduction/)」を確認してください。

## クラスターの作成

Azure Databricks は、Apache Spark "クラスター" を使用して複数のノードでデータを並列に処理する分散処理プラットフォームです。** 各クラスターは、作業を調整するドライバー ノードと、処理タスクを実行するワーカー ノードで構成されています。 この演習では、ラボ環境で使用されるコンピューティング リソース (リソースが制約される場合がある) を最小限に抑えるために、*単一ノード* クラスターを作成します。 運用環境では、通常、複数のワーカー ノードを含むクラスターを作成します。

> **ヒント**: Azure Databricks ワークスペースに 13.3 LTS 以降のランタイム バージョンを持つクラスターが既にある場合は、それを使ってこの演習を完了し、この手順をスキップできます。

1. Azure portal で、スクリプトによって作成された **msl-*xxxxxxx*** リソース グループ (または既存の Azure Databricks ワークスペースを含むリソース グループ) に移動します
1. Azure Databricks Service リソース (セットアップ スクリプトを使って作成した場合は、**databricks-*xxxxxxx*** という名前) を選択します。
1. Azure Databricks ワークスペースの [**概要**] ページで、[**ワークスペースの起動**] ボタンを使用して、新しいブラウザー タブで Azure Databricks ワークスペースを開きます。サインインを求められた場合はサインインします。

    > **ヒント**: Databricks ワークスペース ポータルを使用すると、さまざまなヒントと通知が表示される場合があります。 これらは無視し、指示に従ってこの演習のタスクを完了してください。

1. 左側のサイドバーで、**[(+) 新規]** タスクを選択し、**[クラスター]** を選択します。
1. **[新しいクラスター]** ページで、次の設定を使用して新しいクラスターを作成します。
    - **クラスター名**: "ユーザー名の" クラスター (既定のクラスター名)**
    - **ポリシー**:Unrestricted
    - **クラスター モード**: 単一ノード
    - **アクセス モード**: 単一ユーザー (*自分のユーザー アカウントを選択*)
    - **Databricks Runtime のバージョン**: 13.3 LTS (Spark 3.4.1、Scala 2.12) 以降
    - **Photon Acceleration を使用する**: 選択済み
    - **ノードの種類**: Standard_DS3_v2
    - **非アクティブ状態が ** *20* ** 分間続いた後終了する**

1. クラスターが作成されるまで待ちます。 これには 1、2 分かかることがあります。

> **注**: クラスターの起動に失敗した場合、Azure Databricks ワークスペースがプロビジョニングされているリージョンでサブスクリプションのクォータが不足していることがあります。 詳細については、「[CPU コアの制限によってクラスターを作成できない](https://docs.microsoft.com/azure/databricks/kb/clusters/azure-core-limit)」を参照してください。 その場合は、ワークスペースを削除し、別のリージョンに新しいワークスペースを作成してみてください。 次のように、セットアップ スクリプトのパラメーターとしてリージョンを指定できます: `./mslearn-databricks/setup.ps1 eastus`

## Spark を使用してデータ ファイルを分析する

多くの Spark 環境と同様に、Databricks では、ノートブックを使用して、データの探索に使用できるノートと対話型のコード セルを組み合わせることができます。

1. サイド バーで **[(+) 新規]** タスクを使用して、**Notebook** を作成します。
1. 既定のノートブック名 (**Untitled Notebook *[日付]***) を「**製品の探索**」に変更し、**[接続]** のドロップダウン リストで、まだ選択されていない場合はクラスターを選択します。 クラスターが実行されていない場合は、起動に 1 分ほどかかる場合があります。
1. [**products.csv**](https://raw.githubusercontent.com/MicrosoftLearning/mslearn-databricks/main/data/products.csv) ファイルを `https://raw.githubusercontent.com/MicrosoftLearning/mslearn-databricks/main/data/products.csv` からローカル コンピューターにダウンロードし、**products.csv** として保存します。 次に、**製品の探索** ノートブックで、 **[ファイル]** メニューの **[DBFS へのデータのアップロード]** を選択します。
1. **[データのアップロード]** ダイアログ ボックスで、ファイルのアップロード先の **DBFS ターゲット ディレクトリ**をメモします。 次に、 **[ファイル]** 領域を選択し、ダウンロードした **products.csv** ファイルをコンピューターにアップロードします。 ファイルがアップロードされたら、 **[次へ]** を選択します
1. **[ノートブックからファイルにアクセスする]** ペインで、サンプルの PySpark コードを選択して、クリップボードにコピーします。 これは、ファイルから DataFrame にデータを読み込むのに使用します。 **[完了]** を選択します。
1. 「**製品の探索**」ノートブックの空のコード セルに、コピーしたコードを貼り付けます。次のようになります。

    ```python
    df1 = spark.read.format("csv").option("header", "true").load("dbfs:/FileStore/shared_uploads/user@outlook.com/products.csv")
    ```

1. ダイアログが表示されたら、セルの右上にある  **[&#9656; セルの実行]** メニュー オプションを使用して実行し、クラスターを起動してアタッチします。
1. コードによって実行される Spark ジョブが完了するまで待ちます。 コードによって、アップロードしたファイルのデータから **df1** という名前の "データフレーム" オブジェクトが作成されました。**
1. 既存のコード セルの下で、 **[+]** アイコンを使用して新しいコード セルを追加します。 この新しいセルに次のコードを入力します。

    ```python
   display(df1)
    ```

1. 新しいセルの右上にある **[&#9656; セルの実行]** メニュー オプションを使用して実行します。 このコードを実行して、データフレームの内容を表示します。次のように表示されるはずです。

    | ProductID | ProductName | カテゴリ | ListPrice |
    | -- | -- | -- | -- |
    | 771 | Mountain-100 Silver, 38 | マウンテン バイク | 3399.9900 |
    | 772 | Mountain-100 Silver, 42 | マウンテン バイク | 3399.9900 |
    | ... | ... | ... | ... |

1. 結果の表の上にある **[+]** 、 **[視覚化]** の順に選択して視覚化エディターを表示し、次のオプションを適用します。
    - **視覚化の種類**: 横棒
    - **X 列**: カテゴリ
    - **Y 列**: "新しい列を追加し、" **ProductID** "を選択します"。** **Count** "集計を適用します"。** **

    視覚化を保存し、次のようにノートブックに表示されることを確認します。

    ![カテゴリ別の製品数を示す横棒グラフ](./images/databricks-chart.png)

## テーブルを作成してクエリを実行する

多くのデータ分析はファイル内のデータを操作するのに Python や Scala などの言語を使用するのが楽ですが、数多くのデータ分析ソリューションが、データがテーブルに格納され、SQL を使用して操作するリレーショナル データベース上に構築されています。

1. 「**製品の探索**」ノートブックで、先ほど実行したコード セルからのグラフ出力の下で、 **[+]** アイコンを使用して新しいセルを追加します。
2. この新しいセルに次のコードを入力して実行します。

    ```python
   df1.write.saveAsTable("products")
    ```

3. セルが完了したら、その下に新しいセルを追加して、次のコードを入力します。

    ```sql
   %sql

   SELECT ProductName, ListPrice
   FROM products
   WHERE Category = 'Touring Bikes';
    ```

4. この新しいセルを実行します。*Touring Bikes* カテゴリの製品の名前と価格を返す SQL コードが含まれます。
5. サイド バーで **[カタログ]** のタスクを選択し、**products** テーブルが既定のデータベース スキーマ (当然ながら、「**default**」という名前) 内に作成されていることを確認します。 Spark コードを使用して、データ アナリストがデータの探索と分析レポートの生成に使用できるカスタム データベース スキーマとリレーショナル テーブルのスキーマを作成できます。

## クリーンアップ

Azure Databricks ポータルの **[コンピューティング]** ページでクラスターを選択し、**[&#9632; 終了]** を選択してクラスターをシャットダウンします。

Azure Databricks を調べ終わったら、作成したリソースを削除できます。これにより、不要な Azure コストが生じないようになり、サブスクリプションの容量も解放されます。
